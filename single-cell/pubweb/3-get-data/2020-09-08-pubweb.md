# Pubweb + NF

## Get-data step

Start up a VM. I've been using Ubuntu 18.04 as the OS since it's stable. Since I'll be running pubweb code, get something with 8 cores & 100GiB+ of storage. I chose an m5ad.2xlarge

```bash

ssh -i "dnambi-vm-key-sttr.pem" ubuntu@ec2-34-208-18-92.us-west-2.compute.amazonaws.com

```

Instance storage

```bash
fdisk -l | grep /dev
mkdir -p /data
mkfs.ext4 /dev/nvme0n1
mount -t ext4 /dev/nvme0n1 /data
```


Make the necessary folders

```bash
mkdir -p /data/input
mkdir -p /data/output
mkdir -p /data/scratch
mkdir -p /data/work
mkdir -p /data/wf

```

Copy in the nextflow script

```bash
mkdir -p /data/wf
cd /data/wf
nano #main.nf
nano #pubwebfiles.json
nano #local.config

```


Save this as ```cd /data/wf/rerun.sh```

```bash

TARGET_DIR=/data/output
SOURCE_DIR=/data/input
NEXTFLOW_PARAMS="--s3target $TARGET_DIR --s3source $SOURCE_DIR"
echo "Nextflow params are $NEXTFLOW_PARAMS"

nextflow run main.nf $NEXTFLOW_PARAMS --wfconfig pubwebfiles.json --C local.config

```

Run the script

```bash
chmod +x rerun.sh

. rerun.sh

```

Jq changes:

```bash

cat pubwebfiles.json | jq '.parameters.input.source | map(.name), map(.file)'

cat pubwebfiles.json | jq -r '.parameters.input.source | map( [.name, .file] | join(", ")) | join("\n")' > input.csv

while IFS="," read -r name url
do
  echo "Now getting $name from $url"
  curl $url -o $name
done < input.csv

```

Find and tarball output


```bash

find . -type f -print | tar -czf ignore/input.tar.gz --files-from -

```












