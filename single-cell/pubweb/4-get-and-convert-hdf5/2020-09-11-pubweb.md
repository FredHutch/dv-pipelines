# Pubweb + NF - Get-Data + Convert-to-HDF5

## Get-data step

Start up a VM. I've been using Ubuntu 18.04 as the OS since it's stable. Since I'll be running pubweb code, get something with 8 cores & 100GiB+ of storage. I chose an m5ad.2xlarge

```bash

ssh -i "dnambi-vm-key-sttr.pem" ubuntu@ec2-34-208-18-92.us-west-2.compute.amazonaws.com

ssh -i "dnambi-vm-key-sttr.pem" ubuntu@ec2-44-234-32-205.us-west-2.compute.amazonaws.com


```


### Install Docker

```bash

sudo -i
apt -y update
apt-get -y install docker.io awscli

```

Instance storage

```bash
fdisk -l | grep /dev
mkdir -p /data
mkfs.ext4 /dev/nvme0n1
mount -t ext4 /dev/nvme0n1 /data
```



### Make the container

copy in the Dockerfile

```bash
cd $HOME
mkdir -p container
cd container
nano #copy in the Dockerfile
nano #copy in startwhile.sh
```

Build the container

```bash
docker build -t hdf5 .
docker images | grep hdf5

screen

docker run -v /data:/data hdf5

screen -a+D

docker exec -ti d443f2462fc0 /bin/bash

docker exec -ti 3db91766a25c /bin/bash
```



## Run it on AWS Batch


Change around the pubwebfiles.json
Change the nextflow.config

Change the rerun.sh

```bash

TARGET_DIR=s3://test-nextflow-data/5a30f3af-2d54-4c92-a0b7-dcbb94c145e7/
SOURCE_DIR=/s3://test-nextflow-data/5a30f3af-2d54-4c92-a0b7-dcbb94c145e7/
NEXTFLOW_PARAMS="--s3target $TARGET_DIR --s3source $SOURCE_DIR"
echo "Nextflow params are $NEXTFLOW_PARAMS"

nextflow run main.nf $NEXTFLOW_PARAMS --wfconfig pubwebfiles.json --C nextflow.config

```








