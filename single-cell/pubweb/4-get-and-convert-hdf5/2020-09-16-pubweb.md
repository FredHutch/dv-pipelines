# Pubweb + NF - Get-Data-and-Convert-to-HDF5

## Get-data step

Start up a VM. I've been using Ubuntu 18.04 as the OS since it's stable. Since I'll be running pubweb code, get something with 8 cores & 200GiB+ of storage. I chose an i3.xlarge

```bash



```


### Install Docker

```bash

sudo -i
apt -y update
apt-get -y install docker.io awscli

```

Instance storage

```bash
fdisk -l | grep /dev
mkdir -p /data
mkfs.ext4 /dev/nvme0n1
mount -t ext4 /dev/nvme0n1 /data
```



### Make the container

copy in the Dockerfile

```bash
cd $HOME
mkdir -p container
cd container
nano #copy in the Dockerfile
nano #copy in startwhile.sh
```

Build the container

```bash
docker build -t hdf5 .
docker images | grep hdf5
```


Run the container

```bash
screen

docker run -v /data:/data hdf5

screen -a+D

screen

docker exec -ti d443f2462fc0 /bin/bash

screen -a+D
```



## Run it on AWS Batch


Copy in the nextflow script

```bash
mkdir -p /data/wf
cd /data/wf
nano #main.nf
nano #pubwebfiles.json
nano #local.config

```


```bash
# use a new GUID for testing purposes
TARGET_DIR=s3://test-nextflow-data/5447ac53-e091-459d-9e14-31297e1fa1f4
SOURCE_DIR=s3://test-nextflow-data/5447ac53-e091-459d-9e14-31297e1fa1f4
NEXTFLOW_PARAMS="--s3target $TARGET_DIR --s3source $SOURCE_DIR"
echo "Nextflow params are $NEXTFLOW_PARAMS"

nextflow run main.nf $NEXTFLOW_PARAMS --wfconfig pubwebfiles.json --C nextflow.config

```








