# Pubweb + NF


## Check on the too-long runtime:


bash -o pipefail -c trap "{ ret=$?; aws --region us-west-2 s3 cp --only-show-errors .command.log s3://test-nextflow-data/scratch/e1/a4acc124cbbcc6de33a639d61bd707/.command.log||true; exit $ret; }" EXIT; aws --region us-west-2 s3 cp --only-show-errors s3://test-nextflow-data/scratch/e1/a4acc124cbbcc6de33a639d61bd707/.command.run - | bash 2>&1 | tee .command.log





















Copy the code up to S3 in development, to the ```dv-code-dev``` bucket:

```bash
cd GitHub/data-vizualization-center/hdf5 # wherever this is in your computer

aws s3 rm s3://dv-code-dev/pubweb --recursive
aws s3 cp pubweb/ s3://dv-code-dev/pubweb/ --recursive

```

Get the code from prod, do a diff with the code in dev.
What's changed? Loops? Infinite runtimes? WTF? 


## Connect to the already-existing docker container

Start up a VM. I've been using Ubuntu 18.04 as the OS since it's stable. Since I'll be running pubweb code, get something with 8 cores & 100GiB+ of storage. I chose an m5ad.2xlarge


```bash

ssh -i "dnambi-vm-key-sttr.pem" ubuntu@ec2-35-167-125-241.us-west-2.compute.amazonaws.com


```


### Install Docker

```bash

sudo -i
apt -y update
apt-get -y install docker.io awscli

```

Instance storage

```bash
fdisk -l | grep /dev
mkdir -p /data
mkfs.ext4 /dev/nvme0n1
mount -t ext4 /dev/nvme0n1 /data
```

### Make the container

copy in the Dockerfile

```bash
cd $HOME
mkdir -p container
cd container
nano #copy in the Dockerfile
nano #copy in startwhile.sh
```

Build the container

```bash
docker build -t hdf5 .
docker images | grep hdf5

screen

docker run -v /data:/data hdf5

screen -a+D

docker exec -ti d443f2462fc0 /bin/bash
```

Copy down the source data for the PUBWEB NF step:


```bash
mkdir -p /data
cd /data

aws s3 cp s3://test-nextflow-data/e7f584cc-8105-43eb-a22e-836aa026e505/20200820/output.hdf5 . 

tar -czvf ../input.tar.gz .

rm output.hdf5

```

Make the necessary folders

```bash
mkdir -p /data/input
mkdir -p /data/output
mkdir -p /data/scratch
mkdir -p /data/work

mv ../input.tar.gz /data/input

```


Copy in the nextflow script

```bash
mkdir -p /data/wf
cd /data/wf
nano #main.nf
nano #pubwebconf.json

```

Save this as ```cd /data/wf/rerun.sh```

```bash

TARGET_DIR=/data/output
SOURCE_DIR=/data/input
NEXTFLOW_PARAMS="--s3target $TARGET_DIR --s3source $SOURCE_DIR"
echo "Nextflow params are $NEXTFLOW_PARAMS"

nextflow run main.nf $NEXTFLOW_PARAMS --wfconfig pubwebfiles.json --C local.config

```

Run the script

```bash
chmod +x rerun.sh

. rerun.sh

```










## Set up another container to run it on AWS

```bash
screen

docker run --name="hdf5v2" -v /data/aws:/data/aws hdf5 

Ctrl-a+d

docker ps

docker exec -ti 56eb9036c0fb /bin/bash

```


Copy in the nextflow script

```bash
mkdir -p $HOME/wf
cd $HOME/wf
nano #mmain-4-test-pubweb-aws.nf
nano #pubwebfiles.json
nano #nextflow.config - does the AWS batch config work

```

Copy the input.tar.gz file up to S3:

```bash

aws s3 cp input.tar.gz s3://test-nextflow-data/e7f584cc-8105-43eb-a22e-836aa026e505/20200902/

```


Save this as ```cd /data/wf/rerun.sh```

```bash

TARGET_DIR=s3://test-nextflow-data/e7f584cc-8105-43eb-a22e-836aa026e505/20200902/output
SOURCE_DIR=s3://test-nextflow-data/e7f584cc-8105-43eb-a22e-836aa026e505/20200902
NEXTFLOW_PARAMS="--s3target $TARGET_DIR --s3source $SOURCE_DIR"
echo "Nextflow params are $NEXTFLOW_PARAMS"

nextflow run main.nf $NEXTFLOW_PARAMS --wfconfig pubwebfiles.json --C nextflow.config

```

Run the script

```bash
chmod +x rerun.sh

. rerun.sh

```






## Next up, the CONVERT_HDF5 STEP:


Make an input tarball

Source files are at s3://test-nextflow-data/5a30f3af-2d54-4c92-a0b7-dcbb94c145e7/











